{"cells":[{"cell_type":"code","source":["pip install nltk"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30f2708b-c543-4f98-903f-1e1e467166ba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Python interpreter will be restarted.\nRequirement already satisfied: nltk in /local_disk0/.ephemeral_nfs/envs/pythonEnv-37354e6d-b6ab-4058-b256-520e61964e8d/lib/python3.8/site-packages (3.7)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-37354e6d-b6ab-4058-b256-520e61964e8d/lib/python3.8/site-packages (from nltk) (4.64.0)\nRequirement already satisfied: click in /local_disk0/.ephemeral_nfs/envs/pythonEnv-37354e6d-b6ab-4058-b256-520e61964e8d/lib/python3.8/site-packages (from nltk) (8.1.3)\nRequirement already satisfied: regex>=2021.8.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-37354e6d-b6ab-4058-b256-520e61964e8d/lib/python3.8/site-packages (from nltk) (2022.6.2)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.8/site-packages (from nltk) (1.0.1)\nPython interpreter will be restarted.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Python interpreter will be restarted.\nRequirement already satisfied: nltk in /local_disk0/.ephemeral_nfs/envs/pythonEnv-37354e6d-b6ab-4058-b256-520e61964e8d/lib/python3.8/site-packages (3.7)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-37354e6d-b6ab-4058-b256-520e61964e8d/lib/python3.8/site-packages (from nltk) (4.64.0)\nRequirement already satisfied: click in /local_disk0/.ephemeral_nfs/envs/pythonEnv-37354e6d-b6ab-4058-b256-520e61964e8d/lib/python3.8/site-packages (from nltk) (8.1.3)\nRequirement already satisfied: regex>=2021.8.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-37354e6d-b6ab-4058-b256-520e61964e8d/lib/python3.8/site-packages (from nltk) (2022.6.2)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.8/site-packages (from nltk) (1.0.1)\nPython interpreter will be restarted.\n"]}}],"execution_count":0},{"cell_type":"code","source":["import nltk\nimport re\n\nfrom nltk.corpus import stopwords\nfrom nltk.tag import pos_tag\n\nnltk.download(\"stopwords\")\nnltk.download('averaged_perceptron_tagger')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3188244-db26-49df-bf4a-23104541b362"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /root/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\nOut[1]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /root/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\nOut[1]: True"]}}],"execution_count":0},{"cell_type":"code","source":["#Load the file \nlittlewomen = sc.textFile(\"/FileStore/tables/Little_Women.txt\")\nlittlewomen.take(20)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"601b34a9-2262-4a8c-8ce1-11b4c269ea6f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[2]: ['The Project Gutenberg eBook of Little Women, by Louisa May Alcott',\n '',\n 'This eBook is for the use of anyone anywhere in the United States and',\n 'most other parts of the world at no cost and with almost no restrictions',\n 'whatsoever. You may copy it, give it away or re-use it under the terms',\n 'of the Project Gutenberg License included with this eBook or online at',\n 'www.gutenberg.org. If you are not located in the United States, you',\n 'will have to check the laws of the country where you are located before',\n 'using this eBook.',\n '',\n 'Title: Little Women',\n '',\n 'Author: Louisa May Alcott',\n '',\n 'Release Date: May, 1996 [eBook #514]',\n '[Most recently updated: November 27, 2021]',\n '',\n 'Language: English',\n '',\n '']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[2]: ['The Project Gutenberg eBook of Little Women, by Louisa May Alcott',\n '',\n 'This eBook is for the use of anyone anywhere in the United States and',\n 'most other parts of the world at no cost and with almost no restrictions',\n 'whatsoever. You may copy it, give it away or re-use it under the terms',\n 'of the Project Gutenberg License included with this eBook or online at',\n 'www.gutenberg.org. If you are not located in the United States, you',\n 'will have to check the laws of the country where you are located before',\n 'using this eBook.',\n '',\n 'Title: Little Women',\n '',\n 'Author: Louisa May Alcott',\n '',\n 'Release Date: May, 1996 [eBook #514]',\n '[Most recently updated: November 27, 2021]',\n '',\n 'Language: English',\n '',\n '']"]}}],"execution_count":0},{"cell_type":"code","source":["#split the file into words of length greater than 0\nlittlewomen_split = littlewomen.flatMap(lambda x: re.split(\"[\\[\\]# ./\\\"'-:;|?*“”’‘]\", x)).filter(lambda x: len(x) > 0)\nlittlewomen_split.take(15)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6ac62f2-edeb-4a65-abf6-ce4439e57bdf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[3]: ['The',\n 'Project',\n 'Gutenberg',\n 'eBook',\n 'of',\n 'Little',\n 'Women',\n 'by',\n 'Louisa',\n 'May',\n 'Alcott',\n 'This',\n 'eBook',\n 'is',\n 'for']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[3]: ['The',\n 'Project',\n 'Gutenberg',\n 'eBook',\n 'of',\n 'Little',\n 'Women',\n 'by',\n 'Louisa',\n 'May',\n 'Alcott',\n 'This',\n 'eBook',\n 'is',\n 'for']"]}}],"execution_count":0},{"cell_type":"code","source":["#filter stopwords using function from nltk library\nlittlewomen_filtered = littlewomen_split.filter(lambda x: x.lower() not in stopwords.words(\"english\"))\nlittlewomen_filtered.take(15)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ddf7b83e-3590-41fd-b9da-7728e4e00125"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPicklingError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-4463470365468534>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m#filter stopwords using function from nltk library\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mlittlewomen_filtered\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlittlewomen_split\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlower\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mstopwords\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwords\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"english\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mlittlewomen_filtered\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtake\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m15\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mtake\u001B[0;34m(self, num)\u001B[0m\n\u001B[1;32m   1598\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1599\u001B[0m             \u001B[0mp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpartsScanned\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpartsScanned\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mnumPartsToTry\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtotalParts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1600\u001B[0;31m             \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrunJob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtakeUpToNumLeft\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1601\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1602\u001B[0m             \u001B[0mitems\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mres\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/context.py\u001B[0m in \u001B[0;36mrunJob\u001B[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001B[0m\n\u001B[1;32m   1333\u001B[0m             \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1334\u001B[0m                 \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mremove\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1335\u001B[0;31m         \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrunJob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmappedRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpartitions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1336\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmappedRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1337\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36m_jrdd\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2983\u001B[0m             \u001B[0mprofiler\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2984\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2985\u001B[0;31m         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n\u001B[0m\u001B[1;32m   2986\u001B[0m                                       self._jrdd_deserializer, profiler)\n\u001B[1;32m   2987\u001B[0m         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36m_wrap_function\u001B[0;34m(sc, func, deserializer, serializer, profiler)\u001B[0m\n\u001B[1;32m   2862\u001B[0m     \u001B[0;32massert\u001B[0m \u001B[0mserializer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"serializer should not be empty\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2863\u001B[0m     \u001B[0mcommand\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprofiler\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdeserializer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2864\u001B[0;31m     \u001B[0mpickled_command\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbroadcast_vars\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mincludes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_prepare_for_python_RDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2865\u001B[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001B[1;32m   2866\u001B[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36m_prepare_for_python_RDD\u001B[0;34m(sc, command)\u001B[0m\n\u001B[1;32m   2848\u001B[0m     \u001B[0;31m# the serialized command will be compressed by broadcast\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2849\u001B[0m     \u001B[0mser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCloudPickleSerializer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2850\u001B[0;31m     \u001B[0mpickled_command\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mser\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdumps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2851\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpickled_command\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetBroadcastThreshold\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# Default 1M\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2852\u001B[0m         \u001B[0;31m# The broadcast will have same life cycle as created PythonRDD\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/serializers.py\u001B[0m in \u001B[0;36mdumps\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    471\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdumps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    472\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 473\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mcloudpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdumps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpickle_protocol\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    474\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPickleError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    475\u001B[0m             \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\u001B[0m in \u001B[0;36mdumps\u001B[0;34m(obj, protocol, buffer_callback)\u001B[0m\n\u001B[1;32m     71\u001B[0m                 \u001B[0mfile\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprotocol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuffer_callback\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbuffer_callback\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     72\u001B[0m             )\n\u001B[0;32m---> 73\u001B[0;31m             \u001B[0mcp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdump\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     74\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mfile\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetvalue\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\u001B[0m in \u001B[0;36mdump\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    561\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdump\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    562\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 563\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mPickler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdump\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    564\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mRuntimeError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    565\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;34m\"recursion\"\u001B[0m \u001B[0;32min\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mPicklingError\u001B[0m: args[0] from __newobj__ args has the wrong class","errorSummary":"<span class='ansi-red-fg'>PicklingError</span>: args[0] from __newobj__ args has the wrong class","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPicklingError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-4463470365468534>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m#filter stopwords using function from nltk library\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mlittlewomen_filtered\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlittlewomen_split\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlower\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mstopwords\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwords\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"english\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mlittlewomen_filtered\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtake\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m15\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mtake\u001B[0;34m(self, num)\u001B[0m\n\u001B[1;32m   1598\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1599\u001B[0m             \u001B[0mp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpartsScanned\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpartsScanned\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mnumPartsToTry\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtotalParts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1600\u001B[0;31m             \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrunJob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtakeUpToNumLeft\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1601\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1602\u001B[0m             \u001B[0mitems\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mres\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/context.py\u001B[0m in \u001B[0;36mrunJob\u001B[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001B[0m\n\u001B[1;32m   1333\u001B[0m             \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1334\u001B[0m                 \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mremove\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1335\u001B[0;31m         \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrunJob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmappedRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpartitions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1336\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmappedRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1337\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36m_jrdd\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2983\u001B[0m             \u001B[0mprofiler\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2984\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2985\u001B[0;31m         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n\u001B[0m\u001B[1;32m   2986\u001B[0m                                       self._jrdd_deserializer, profiler)\n\u001B[1;32m   2987\u001B[0m         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36m_wrap_function\u001B[0;34m(sc, func, deserializer, serializer, profiler)\u001B[0m\n\u001B[1;32m   2862\u001B[0m     \u001B[0;32massert\u001B[0m \u001B[0mserializer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"serializer should not be empty\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2863\u001B[0m     \u001B[0mcommand\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprofiler\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdeserializer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2864\u001B[0;31m     \u001B[0mpickled_command\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbroadcast_vars\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mincludes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_prepare_for_python_RDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2865\u001B[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001B[1;32m   2866\u001B[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36m_prepare_for_python_RDD\u001B[0;34m(sc, command)\u001B[0m\n\u001B[1;32m   2848\u001B[0m     \u001B[0;31m# the serialized command will be compressed by broadcast\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2849\u001B[0m     \u001B[0mser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCloudPickleSerializer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2850\u001B[0;31m     \u001B[0mpickled_command\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mser\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdumps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2851\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpickled_command\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetBroadcastThreshold\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# Default 1M\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2852\u001B[0m         \u001B[0;31m# The broadcast will have same life cycle as created PythonRDD\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/serializers.py\u001B[0m in \u001B[0;36mdumps\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    471\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdumps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    472\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 473\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mcloudpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdumps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpickle_protocol\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    474\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPickleError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    475\u001B[0m             \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\u001B[0m in \u001B[0;36mdumps\u001B[0;34m(obj, protocol, buffer_callback)\u001B[0m\n\u001B[1;32m     71\u001B[0m                 \u001B[0mfile\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprotocol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuffer_callback\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbuffer_callback\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     72\u001B[0m             )\n\u001B[0;32m---> 73\u001B[0;31m             \u001B[0mcp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdump\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     74\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mfile\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetvalue\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\u001B[0m in \u001B[0;36mdump\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    561\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdump\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    562\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 563\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mPickler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdump\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    564\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mRuntimeError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    565\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;34m\"recursion\"\u001B[0m \u001B[0;32min\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mPicklingError\u001B[0m: args[0] from __newobj__ args has the wrong class"]}}],"execution_count":0},{"cell_type":"code","source":["#filter only the text where part of speech is a noun\nnamed_entities = littlewomen_filtered.filter(lambda x: nltk.pos_tag([x])[0][1] == \"NN\")\nnamed_entities.take(15)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"630fbe9b-2a14-4e4b-8149-2dd3d4717ca6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[5]: ['Project',\n 'eBook',\n 'Alcott',\n 'eBook',\n 'use',\n 'anyone',\n 'world',\n 'cost',\n 'whatsoever',\n 'copy',\n 'use',\n 'Project',\n 'License',\n 'eBook',\n 'online']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[5]: ['Project',\n 'eBook',\n 'Alcott',\n 'eBook',\n 'use',\n 'anyone',\n 'world',\n 'cost',\n 'whatsoever',\n 'copy',\n 'use',\n 'Project',\n 'License',\n 'eBook',\n 'online']"]}}],"execution_count":0},{"cell_type":"code","source":["#make a map out of the words and their appearances and then aggregate to count their frequencies, sorting in descending order\nsorted_freqs = named_entities.map(lambda x: (x.lower(), 1)).reduceByKey(lambda x,y:x+y).sortBy(lambda x: -x[1])\nsorted_freqs.take(15) "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58f38803-d40e-4f73-82a6-a8f738221290"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[6]: [('jo', 1347),\n ('meg', 682),\n ('amy', 647),\n ('laurie', 593),\n ('mother', 368),\n ('time', 321),\n ('mr', 310),\n ('think', 275),\n ('home', 269),\n ('face', 252),\n ('dear', 248),\n ('mrs', 244),\n ('look', 235),\n ('day', 235),\n ('way', 213)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[6]: [('jo', 1347),\n ('meg', 682),\n ('amy', 647),\n ('laurie', 593),\n ('mother', 368),\n ('time', 321),\n ('mr', 310),\n ('think', 275),\n ('home', 269),\n ('face', 252),\n ('dear', 248),\n ('mrs', 244),\n ('look', 235),\n ('day', 235),\n ('way', 213)]"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24f687b3-2652-4f95-b475-e0cee2fb3cd2"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Part-1","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4463470365468528}},"nbformat":4,"nbformat_minor":0}
